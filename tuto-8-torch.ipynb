{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "# Pytorch 101: An almost-comprehensive Pytorch tutorial for beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__N.B.__: This tutorial is using PyTorch version 0.3.1, which is currently installed in Caltech GPU cluster. The newest release, 0.4.0, brings some fundamental changes, which I will mention in details of this tutorial. <br>\n",
    "\n",
    "Author: *Thong Q. Nguyen* <thong@caltech.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. <a href='#intro'> Getting started</a><br>\n",
    "    a. <a href='#whatis'> Why PyTorch?</a><br>\n",
    "    b. <a href='#tensor'> Tensors</a><br>\n",
    "    c. <a href='#variable'> Variables</a><br>\n",
    "2. <a href='#build'> Building a neural network</a><br>\n",
    "    a. <a href='#module'> Modules and Parameters</a><br>\n",
    "    b. <a href='#loss'> Loss function</a><br>\n",
    "    c. <a href='#optim'> Optimizers</a><br>\n",
    "    d. <a href='#dummy'> A dummy model</a><br>\n",
    "    e. <a href='#remark'> Remark on training vs. validation</a><br>\n",
    "3. <a href='#data'> Loading data</a><br>\n",
    "    a. <a href='#dataset'> Dataset class</a><br>\n",
    "    b. <a href='#dataloader'> DataLoader</a><br>\n",
    "4. <a href='#parallel'>Parallel and distributed training</a><br>\n",
    "    a. <a href='#datapara'>Data-parallel training</a><br>\n",
    "    b. <a href='#distributed'>Distributed training</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i. <a href='#backend'>Backend</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ii. <a href='#world'>World size and rank</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iii. <a href='#init'>Distributed package initialization</a><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iv. <a href='#distmodel'>Distributed model and data</a><br>\n",
    "5. <a href='#conclusions'>Final words</a><br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "## Getting started\n",
    "\n",
    "\n",
    "<a id='whatis'></a>\n",
    "### Why PyTorch?\n",
    "PyTorch is a python-based scientific computing package serving 2 purposes:\n",
    "\n",
    "    1. replacement for NumPy to use the power of GPUs\n",
    "    2. deep learning research platform that provides maximum flexibility and speed\n",
    "    \n",
    "PyTorch is widely used in the ML research community (as opposed to in production, where Caffe2 and Tensorflow are more popular). PyTorch offers maximum flexibility to implement new ideas with ease, allowing for a lot of space for algorithmic creativity. Recent DAWNBench competition shows an overwhelming amount of submissions using PyTorch that top the leaderboard in CIFAR10 training https://dawn.cs.stanford.edu/benchmark/CIFAR10/train.html.\n",
    "\n",
    "<a id='tensor'></a>\n",
    "### Tensors\n",
    "The power of PyTorch lies in its <a href='https://pytorch.org/docs/stable/tensors.html'>Tensor class </a>, which is similar to numpy arrays. However, PyTorch tensors (which I will from now on refer to as `Tensors`) can be sent to GPU to accelerate computing. Converting between `Tensors` and numpy arrays and vice versa are extremely easy. Note that the conversion is only possible on CPU. Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1\n",
      "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1,2\" # To select the GPU we want to use and mask out the occupied one\n",
    "print(torch.__version__)\n",
    "import sys\n",
    "print(sys.version) # Make sure we are using Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = \n",
      " 0  0  0\n",
      " 0  0  0\n",
      " 0  0  0\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "y = \n",
      " 0.8678\n",
      " 0.1241\n",
      " 0.5901\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "z = x + y = \n",
      " 0.8678  0.8678  0.8678\n",
      " 0.1241  0.1241  0.1241\n",
      " 0.5901  0.5901  0.5901\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a 3x3 Tensor of zeros\n",
    "x = torch.zeros([3, 3])\n",
    "print(\"x = {}\".format(x))\n",
    "# Create a random 3x1 Tensor \n",
    "y = torch.rand(3,1)\n",
    "print(\"y = {}\".format(y))\n",
    "# Tensor has broadcast property just like numpy \n",
    "z = x + y \n",
    "print(\"z = x + y = {}\".format(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device index: 0. Total devices: 2\n",
      "On GPU: z = \n",
      " 0.8678  0.8678  0.8678\n",
      " 0.1241  0.1241  0.1241\n",
      " 0.5901  0.5901  0.5901\n",
      "[torch.cuda.FloatTensor of size 3x3 (GPU 0)]\n",
      "\n",
      "Notice the type has now become torch.cuda.FloatTensor.\n",
      "Switching to a different device...\n",
      "Current device index: 1\n",
      "On a different GPU: z = \n",
      " 0.8678  0.8678  0.8678\n",
      " 0.1241  0.1241  0.1241\n",
      " 0.5901  0.5901  0.5901\n",
      "[torch.cuda.FloatTensor of size 3x3 (GPU 1)]\n",
      "\n",
      "On CPU: z = \n",
      " 0.8678  0.8678  0.8678\n",
      " 0.1241  0.1241  0.1241\n",
      " 0.5901  0.5901  0.5901\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send Tensor to GPU \n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device index: {}. Total devices: {}\".format(torch.cuda.current_device(), \n",
    "                                                               torch.cuda.device_count()))\n",
    "    torch.cuda.set_device(0)\n",
    "    z = z.cuda()\n",
    "    print(\"On GPU: z = {}\".format(z))\n",
    "    print(\"Notice the type has now become torch.cuda.FloatTensor.\")\n",
    "    # Now let's send it to a different GPU\n",
    "    print(\"Switching to a different device...\")\n",
    "    torch.cuda.set_device(1)\n",
    "    print(\"Current device index: {}\".format(torch.cuda.current_device()))\n",
    "    z = z.cuda()\n",
    "    print(\"On a different GPU: z = {}\".format(z))\n",
    "\n",
    "# Send Tensor back to CPU\n",
    "z = z.cpu()\n",
    "print(\"On CPU: z = {}\".format(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to numpy, z = <class 'numpy.ndarray'>\n",
      "[[0.8677637  0.8677637  0.8677637 ]\n",
      " [0.12407911 0.12407911 0.12407911]\n",
      " [0.59006226 0.59006226 0.59006226]]\n",
      "Converted to Tensor, z = \n",
      " 0.8678  0.8678  0.8678\n",
      " 0.1241  0.1241  0.1241\n",
      " 0.5901  0.5901  0.5901\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "Error: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.\n"
     ]
    }
   ],
   "source": [
    "# Convert Tensor to Numpy array and vice versa\n",
    "import numpy as np\n",
    "z = x + y\n",
    "z = z.numpy()\n",
    "print(\"Converted to numpy, z = {}\\n{}\".format(type(z), z))\n",
    "\n",
    "z = torch.from_numpy(z)\n",
    "print(\"Converted to Tensor, z = {}\".format(z))\n",
    "\n",
    "# Note that the conversion is not possible when the Tensor is on GPU\n",
    "z = z.cuda()\n",
    "try:\n",
    "    z = z.numpy()\n",
    "except RuntimeError as e:\n",
    "    print(\"Error: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='variable'></a>\n",
    "### Variables\n",
    "<a href='#toc'> Back to top </a><br>\n",
    "\n",
    "*Note*: This concept of `Variables` only exists pre-0.4.0. In the 0.4.0 release, `Tensor` and `Variable` has been merged. However, it's good to understand the design philosophy of `Variables` which is fundamental to any neural network.\n",
    "\n",
    "`Variable` is essentially a wrapper of `Tensor` with 1 extra component: gradient. Recall that in a deep neural network, the optimization is done via a process called back-propagation. Given a function $f(\\mathbf{x})$ where $\\mathbf{x}$ is an input vector, we want to compute the gradient of $f$ at $x$, ie, $\\nabla _{x} f(\\mathbf{x})$. This gradient is computed with chain rule backwardly from the loss function. \n",
    "\n",
    "Fortunately, in PyTorch the gradient computation is handled automatically with the automatic differention package called *torch.autograd*. `Variable`, a class in this package, should be all you need for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Variable containing:\n",
      " 0.2907  0.9151\n",
      " 0.6608  0.4161\n",
      " 0.0041  0.5863\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = torch.rand(3,2)\n",
    "x = Variable(x)\n",
    "print(\"x = {}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable has 2 component: Tensor and Gradient. Let's look at each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of x = \n",
      " 0.2907  0.9151\n",
      " 0.6608  0.4161\n",
      " 0.0041  0.5863\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "Gradient of x = None\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensor of x = {}\".format(x.data))\n",
    "print(\"Gradient of x = {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient component is empty right now because there is no computation performed. Knowing these 2 basic concepts, we are ready to move forward to most the exciting part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='build'></a>\n",
    "## Building a neural network\n",
    "<a href='#toc'> Back to top </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just kidding, there are few more concepts you need to know before building a network.\n",
    "\n",
    "<a id='module'></a>\n",
    "### Modules and Parameters\n",
    "`Modules` are the base class for all neural networks. A `Module` typically contains `Parameters`. `Parameters` are similar to `Variables` in the sense that they includes values (weights) and their corresponding gradients. `Parameters` have some extra built-in functions to use in a network's model. `Variables` and `Tensors` are usually just used for input and output data.  \n",
    "\n",
    "<a id='loss'></a>\n",
    "### Loss function\n",
    "Loss function is another module that takes `(prediction, target)` as inputs and computes how far away the prediction is from target. The loss function can then perform a backprop calculation to compute the gradients of the neural network modules. A list of built-in loss functions can be found at https://pytorch.org/docs/0.3.1/nn.html#loss-functions. \n",
    "\n",
    "<a id='optim'></a>\n",
    "### Optimizers\n",
    "The `Optimizer` takes `Parameters` of the model as inputs, and then update the `Parameters` once the gradients have been computed by the loss function's backpropagation. Here is the list of available optimizers https://pytorch.org/docs/0.3.1/optim.html#algorithms.\n",
    "\n",
    "<a id='dummy'></a>\n",
    "### A dummy model\n",
    "<a href='#toc'> Back to top </a>\n",
    "\n",
    "\n",
    "Now you have all the ingradients, let's make an ultra-simple neural network to see how all the pieces are connected together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (dense1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (dense2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a model\n",
    "class Net(nn.Module): ### Network always has to be a subclass of nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(3,5)\n",
    "        self.dense2 = nn.Linear(5,1)\n",
    "    \n",
    "    def forward(self, x): # All the computation steps of the input are defined in this function\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x) # Or you can compact all steps into x = F.relu(self.dense2(F.relu(self.dense1(x)))) \n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look deeper at the model we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "-0.0259  0.4909  0.3690\n",
      " 0.5152  0.2457 -0.4198\n",
      "-0.3849  0.1651  0.5462\n",
      "-0.4988  0.2408  0.0541\n",
      "-0.5344  0.5316  0.1034\n",
      "[torch.FloatTensor of size 5x3]\n",
      ", Parameter containing:\n",
      " 0.2021\n",
      "-0.3067\n",
      "-0.5410\n",
      " 0.2676\n",
      "-0.2391\n",
      "[torch.FloatTensor of size 5]\n",
      ", Parameter containing:\n",
      " 0.2847  0.1915 -0.2046  0.4103  0.0190\n",
      "[torch.FloatTensor of size 1x5]\n",
      ", Parameter containing:\n",
      " 0.4292\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = net.parameters().__next__() # Get the parameter of the first layer, a 5x3 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer value: \n",
      "-0.0259  0.4909  0.3690\n",
      " 0.5152  0.2457 -0.4198\n",
      "-0.3849  0.1651  0.5462\n",
      "-0.4988  0.2408  0.0541\n",
      "-0.5344  0.5316  0.1034\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "First layer gradient: None\n"
     ]
    }
   ],
   "source": [
    "print(\"First layer value: {}\".format(x.data))\n",
    "print(\"First layer gradient: {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a model, which contains 2 dense layers. The first layer is a 5x3 matrix, taking input of dimensionality `(N, C, 3)` where `N` is the batch size and `C` is number of channels. The output of the first layer would be of dimensionality `(N, C, 5)`, which is activated by a RELU function before fed into the second dense layer, a 5x1 matrix. The output is again activated by RELU before returning as the prediction of the given original input.\n",
    "\n",
    "Now we will define our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # We use MSE which is the mean squared error between the predicted output and target.\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having all the ingredients, let's input a fake data point and see how things go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input = Variable containing:\n",
      " 0.4169  0.1048  0.7408\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Target = Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.rand(1,3))\n",
    "target = Variable(torch.ones(1))\n",
    "print(\"Input = {}\".format(input))\n",
    "print(\"Target = {}\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "prediction = net(input) # get the predicted output\n",
    "loss = criterion(prediction, target) # compute the MSE of our prediction with the target \n",
    "loss.backward() # Backprop to compute the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer value: \n",
      "-0.0259  0.4909  0.3690\n",
      " 0.5152  0.2457 -0.4198\n",
      "-0.3849  0.1651  0.5462\n",
      "-0.4988  0.2408  0.0541\n",
      "-0.5344  0.5316  0.1034\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "First layer gradient: Variable containing:\n",
      "-0.0884 -0.0222 -0.1571\n",
      " 0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000\n",
      "-0.1275 -0.0320 -0.2265\n",
      " 0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"First layer value: {}\".format(x.data))\n",
    "print(\"First layer gradient: {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! The layer's weights are unchanged but now we have the gradient, which is also the 5x3 matrix showing how the first layer should change to get closer to the target given the original input.\n",
    "\n",
    "Let's update the weights with newly computed gradients, and looks at the first layer again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer value: \n",
      "-0.0250  0.4911  0.3706\n",
      " 0.5152  0.2457 -0.4198\n",
      "-0.3849  0.1651  0.5462\n",
      "-0.4975  0.2411  0.0564\n",
      "-0.5344  0.5316  0.1034\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "First layer gradient: Variable containing:\n",
      "-0.0884 -0.0222 -0.1571\n",
      " 0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000\n",
      "-0.1275 -0.0320 -0.2265\n",
      " 0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer.step() # Update the weight of the parameters\n",
    "print(\"First layer value: {}\".format(x.data))\n",
    "print(\"First layer gradient: {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row of the matrix has been updated (with learning rate 0.01 we specified earlier). The gradient is still there, this is why we have to call `optimizer.zero_grad()` at every iteration, otherwise new gradients computed in the next step will add up to this old one and result in a crazy weight update. \n",
    "\n",
    "<a id='remark'></a>\n",
    "### Remark on training vs validation\n",
    "<a href='#toc'> Back to top </a>\n",
    "\n",
    "During training, all `Variables` and `Parameters` need to keep track of their gradients, which is quite computationally expensive. During the validation/testing phase, no gradient update is needed. Therefore, we need to set `net.train()` during training and `net.eval()` during testing. \n",
    "\n",
    "When wrapping the inputs and targets into `Variables` during the validation/testing phase, you need to set the `volatile` parameter, such as `inputs, targets = Variable(inputs, volatile=True), Variable(targets,volatile=True)` so that the memory won't explode during validation. Again, this is only true for pre-0.3.5 versions of PyTorch.\n",
    "\n",
    "---\n",
    "Hooray!!! We just went through all basic concepts of PyTorch, including tensor, gradient, module, optimizer, loss function and observed how they work together. However, there is still one important component you need to learn before applying to your own project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loading'></a>\n",
    "## Loading Data\n",
    "<a href='#toc'> Back to top </a>\n",
    "\n",
    "Generally, your dataset would be too big to fit in your device's memory and we have to write a data <a href=\"https://wiki.python.org/moin/Generators\">generator</a> to feed small portions of the dataset to our network iteratively. Fortunately, there is built-in class in PyTorch that does this for us and all we need to do is to write our own Dataset class and pass it to the built-in generator class (DataLoader). \n",
    "\n",
    "<a id='dataset'></a>\n",
    "### Dataset class\n",
    "`torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    "     - __len__ so that len(dataset) returns the size of the dataset.\n",
    "     - __getitem__ to support the indexing such that dataset[i] can be used to get ith sample\n",
    "     \n",
    "<a id='dataloader'></a>\n",
    "### DataLoader \n",
    "<a href='#toc'> Back to top </a><br>\n",
    "`torch.utils.data.DataLoader` is an generator which takes our customized Dataset class as input and outputs each batch of dataset with the batch size specified by our own.\n",
    "\n",
    "The following example is excerpted from my own project. The dataset is stored in multiple HDF5 files located at `/bigdata/shared/LCDJets_Abstract_IsoLep_lt_20`. Each file in the dataset has 2 keys `Images` and `Labels`, where `Images` is a numpy array of size (750, 150, 94, 5) -- read: 750 images of size (150, 94, 5), where 5 is the number of channels and (150x94) is the WxH dimensions of the images. `Labels` are the 1-hot encoded target of the event types: QCD, $t \\bar{t}$, or $W$+jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "from glob import glob\n",
    "\n",
    "class EventImage(Dataset):\n",
    "\n",
    "    def check_data(self, file_names): \n",
    "        '''Count the number of events in each file and mark the threshold \n",
    "        boundaries between adjacent indices coming from 2 different files'''\n",
    "        num_data = 0\n",
    "        thresholds = [0]\n",
    "        for in_file_name in file_names:\n",
    "            h5_file = h5py.File( in_file_name, 'r' )\n",
    "            X = h5_file[self.feature_name]\n",
    "            if hasattr(X, 'keys'):\n",
    "                num_data += len(X[X.keys()[0]])\n",
    "                thresholds.append(num_data)\n",
    "            else:\n",
    "                num_data += len(X)\n",
    "                thresholds.append(num_data)\n",
    "            h5_file.close()\n",
    "        return (num_data, thresholds)\n",
    "\n",
    "    def __init__(self, dir_name, feature_name = 'Images', label_name = 'Labels'):\n",
    "        self.feature_name = feature_name\n",
    "        self.label_name = label_name\n",
    "        self.file_names = glob(dir_name+'/*.h5')\n",
    "        self.num_data, self.thresholds = self.check_data(self.file_names)\n",
    "\n",
    "    def is_numpy_array(self, data):\n",
    "        return isinstance(data, np.ndarray)\n",
    "\n",
    "    def get_num_samples(self, data):\n",
    "        \"\"\"Input: dataset consisting of a numpy array or list of numpy arrays.\n",
    "            Output: number of samples in the dataset\"\"\"\n",
    "        if self.is_numpy_array(data):\n",
    "            return len(data)\n",
    "        else:\n",
    "            return len(data[0])\n",
    "\n",
    "    def load_data(self, in_file_name):\n",
    "        \"\"\"Loads numpy arrays from H5 file.\n",
    "            If the features/labels groups contain more than one dataset,\n",
    "            we load them all, alphabetically by key.\"\"\"\n",
    "        h5_file = h5py.File( in_file_name, 'r' )\n",
    "        X = self.load_hdf5_data(h5_file[self.feature_name] )\n",
    "        Y = self.load_hdf5_data(h5_file[self.label_name] )\n",
    "        h5_file.close()\n",
    "        return X,Y\n",
    "    \n",
    "    def load_hdf5_data(self, data):\n",
    "        \"\"\"Returns a numpy array or (possibly nested) list of numpy arrays\n",
    "            corresponding to the group structure of the input HDF5 data.\n",
    "            If a group has more than one key, we give its datasets alphabetically by key\"\"\"\n",
    "        if hasattr(data, 'keys'):\n",
    "            out = [ self.load_hdf5_data( data[key] ) for key in sorted(data.keys()) ]\n",
    "        else:\n",
    "            out = data[:]\n",
    "        return out\n",
    "\n",
    "    def get_data(self, data, idx):\n",
    "        \"\"\"Input: a numpy array or list of numpy arrays.\n",
    "            Gets elements at idx for each array\"\"\"\n",
    "        if self.is_numpy_array(data):\n",
    "            return data[idx]\n",
    "        else:\n",
    "            return [arr[idx] for arr in data]\n",
    "\n",
    "    def get_index(self, idx):\n",
    "        \"\"\"Translate the global index (idx) into local indexes,\n",
    "        including file index and event index of that file\"\"\"\n",
    "        file_index = next(i for i,v in enumerate(self.thresholds) if v > idx)\n",
    "        file_index -= 1\n",
    "        event_index = idx - self.thresholds[file_index]\n",
    "        return file_index, event_index\n",
    "\n",
    "    def get_thresholds(self):\n",
    "        return self.thresholds\n",
    "\n",
    "    # Below are the two functions you are required to define\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_index, event_index = self.get_index(idx)\n",
    "        X, Y = self.load_data(self.file_names[file_index])\n",
    "        return {'Images': self.get_data(X, event_index), 'Labels': np.argmax(self.get_data(Y, event_index))}\n",
    "\n",
    "    \n",
    "# Define the data generators from the training set and validation set. Let's try a batch size of 12.\n",
    "train_set = EventImage(dir_name='/bigdata/shared/LCDJets_Abstract_IsoLep_lt_20//train/',\n",
    "                       feature_name ='Images',label_name = 'Labels')\n",
    "train_loader = DataLoader(train_set, batch_size=12, shuffle=True, num_workers=4)\n",
    "\n",
    "val_set = EventImage(dir_name='/bigdata/shared/LCDJets_Abstract_IsoLep_lt_20//val/',\n",
    "                     feature_name ='Images',label_name = 'Labels')\n",
    "val_loader = DataLoader(val_set, batch_size=12, shuffle=True , num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape = torch.Size([12, 150, 94, 5])\n",
      "Targets shape = torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(train_loader):\n",
    "    inputs, targets = data['Images'], data['Labels']\n",
    "    print(\"Inputs shape = {}\".format(inputs.shape))\n",
    "    print(\"Targets shape = {}\".format(targets.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we got a pair of (input, target) from the data generator with batch size of 12. \n",
    "\n",
    "For an end-to-end training reference, please take a look at https://github.com/pytorch/examples/blob/0.3.1/mnist/main.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parallel'></a>\n",
    "## Parallel and distributed training\n",
    "<a href='#toc'> Back to top </a><br>\n",
    "\n",
    "When dealing with large dataset and complex model, we want to *parallelize* the model on multiple GPUs to speed up the training. In a computer cluster where there are multiple nodes (such as `imperium-sm`, `culture-plate-sm`, and `flere-imsaho-sm` in Caltech GPU cluster), we can also train the model across multiple nodes. In this case it's called *distributed training*. PyTorch supports out-of-the-box setup for parallel and distributed training that takes care of communication between GPUs.  \n",
    "\n",
    "<a id='datapara'></a>\n",
    "### DataParallel\n",
    "<a href='#toc'> Back to top </a><br>\n",
    "\n",
    "In a data parallel setting, the model is copied to multiple GPUs in 1 node. The mini-batch of samples is splitted into multiple smaller mini-batches to send to different GPUs; the model in each GPU performs the computation for this smaller mini-batch in parallel and synchronizes the outputs across all GPUs. \n",
    "\n",
    "The mechanism for data parallel is implemented in `torch.nn.DataParallel` class. We can wrap our model in `DataParallel` and it will be parallelized over multiple GPUs in the batch dimension. The rest of the training code should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): Net(\n",
      "    (dense1): Linear(in_features=3, out_features=5, bias=True)\n",
      "    (dense2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = torch.nn.DataParallel(net).cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distributed'></a>\n",
    "### Distributed training\n",
    "<a href='#toc'> Back to top </a><br>\n",
    "\n",
    "In a distributed setting scenario, we can train the model on multiple GPUs in 1 node just like DataParallel training, but we can also extend it to different nodes of the cluster.\n",
    "\n",
    "A nice example of writing a distributed training program can be found at https://github.com/pytorch/examples/blob/0.3.1/imagenet/main.py. I will go over a few concepts that we need to keep in mind when running a distributed program.\n",
    "\n",
    "<a id='backend'></a>\n",
    "#### Backend\n",
    "PyTorch 0.3.1 supports 3 different backends that manage the communication method across all devices: `tcp`, `gloo`, and `mpi`. Since `tcp` only supports CPU and `mpi` needs customized installation, I recommend using `gloo` backend on Caltech cluster. \n",
    "\n",
    "<a id='world'></a>\n",
    "#### World size and rank\n",
    "In distributed training terminology, `world size` refers to the total number of distributed processes and `rank` represents the indices of the processes. The `master process`, which manages all the `workers`, will always be rank 0. \n",
    "\n",
    "<a id='init'></a>\n",
    "#### Distribution package initialization\n",
    "<a href='#toc'> Back to top </a><br>\n",
    "\n",
    "To start the distributed training program, we need to initialize the distribution package `torch.distributed`. There are many different initialization methods; in this tutorial I will use the environment variable initialization, in which you need to set 2 variables `MASTER_PORT` and `MASTER_ADDR`, which indicate the port and address of the master node. The addresses of each nodes are listed in `/etc/hosts`. For example, if you decide to use `culture-plate-sm` as your master node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_PORT'] = '3466' # any random number would work\n",
    "os.environ['MASTER_ADDR'] = '10.3.10.98' # the address of culture-plate-sm, as listed in /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start the initialization:\n",
    "\n",
    "```python\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(backend='gloo',init_method= 'env://',\n",
    "                        world_size=args.world_size, rank=args.rank)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, `world_size` need to be `N` > 1; however, in this tutorial if we do so the notebook will hang until we spawn enough `N` processes. \n",
    "\n",
    "For example, we decide to set `world_size` = 3. We need to open a new terminal, run the same program again but set `rank` = 1 and then repeat in another terminal with `rank` = 2. In each initialization we can also change `os.environ['CUDA_VISIBLE_DEVICES']` to force each process to use a different GPU. \n",
    "\n",
    "If all processes are within the same node, there is a faster way that doesn't require opening `N` terminals. Write a script named `multiproc.py`:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "argslist = list(sys.argv)[1:]\n",
    "world_size = torch.cuda.device_count()\n",
    "\n",
    "if '--world-size' in argslist:\n",
    "    world_size = int(argslist[argslist.index('--world-size')+1])\n",
    "else:\n",
    "    argslist.append('--world-size')\n",
    "    argslist.append(str(world_size))\n",
    "\n",
    "workers = []\n",
    "\n",
    "for i in range(world_size):\n",
    "    if '--rank' in argslist:\n",
    "        argslist[argslist.index('--rank')+1] = str(i)\n",
    "    else:\n",
    "        argslist.append('--rank')\n",
    "        argslist.append(str(i))\n",
    "    stdout = None if i == 0 else open(\"GPU_\"+str(i)+\".log\", \"w\")\n",
    "    print(argslist)\n",
    "    p = subprocess.Popen([str(sys.executable)]+argslist, stdout=stdout)\n",
    "    workers.append(p)\n",
    "\n",
    "for p in workers:\n",
    "    p.wait()\n",
    "```\n",
    "\n",
    "And then run it with our main program, `main.py`, as the input parameter:\n",
    "\n",
    "```bash\n",
    "python3 multiproc main.py --world-size 3\n",
    "```\n",
    "\n",
    "<a id='distmodel'></a>\n",
    "#### Distributed model and data\n",
    "<a href='#toc'> Back to top </a><br>\n",
    "\n",
    "Similar to `DataParallel`, we need to wrap our model inside `torch.nn.parallel.DistributedDataParallel` class. \n",
    "```python\n",
    "net = Net().cuda()\n",
    "net = nn.parallel.DistributedDataParallel(net)\n",
    "```\n",
    "However, unlike `DataParallel`, we need to modify our data loader to distribute smaller mini-batches to different processes, using `torch.utils.data.distributed.DistributedSampler` class:\n",
    "\n",
    "```python\n",
    "train_set = EventImage(dir_name='/bigdata/shared/LCDJets_Abstract_IsoLep_lt_20/train/', feature_name ='Images', label_name = 'Labels')\n",
    "if args.world_size > 1: # Distributed training mode\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set)\n",
    "else:\n",
    "    train_sampler = None\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=(train_sampler == None), num_workers=args.workers, sampler=train_sampler)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id='conclusions'></a>\n",
    "## Final words\n",
    "<a href='#toc'> Back to top </a><br>\n",
    "\n",
    "Hooray! We just went through a lot, which is certainly not everything, but it can give you a good start to write a deep learning program in PyTorch. The ability to print out anything without open a separated session sets PyTorch in its own league above Keras, Tensorflow, etc. in terms of convenience for debugging. In more complex tasks such as changing the target to a different dataset or modifying the network architecture after a few epochs, it's much easier to do so in PyTorch than in other frameworks. Even if you're not doing deep learning but usual numerical analysis where you have to invert gigantic matrices, writing it in PyTorch (ie, using Tensor instead of NumPy arrays) gives you a strong performance boost with GPUs. For those reasons, learning PyTorch would be a worthwhile investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
